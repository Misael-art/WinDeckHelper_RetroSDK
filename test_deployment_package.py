#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Teste do Pacote de Deployment
Testa se o pacote de deployment foi criado corretamente e funciona.
"""

import os
import sys
import subprocess
import logging
from pathlib import Path
import json
import time

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class DeploymentTester:
    """Testador do pacote de deployment"""
    
    def __init__(self):
        self.project_root = Path.cwd()
        self.deployment_dir = self.project_root / "deployment"
        self.package_dir = None
        self.test_results = {
            "timestamp": time.time(),
            "tests": {},
            "overall_status": "unknown"
        }
    
    def find_package_directory(self):
        """Encontra o diretÃ³rio do pacote"""
        logger.info("ğŸ” Procurando pacote de deployment...")
        
        possible_dirs = [
            self.deployment_dir / "EnvironmentDevDeepEvaluation_Portable",
            self.deployment_dir / "package",
            self.project_root / "dist" / "EnvironmentDevDeepEvaluation"
        ]
        
        for dir_path in possible_dirs:
            if dir_path.exists():
                self.package_dir = dir_path
                logger.info(f"  âœ… Pacote encontrado: {dir_path}")
                return True
        
        logger.error("  âŒ Pacote nÃ£o encontrado")
        return False
    
    def test_package_structure(self):
        """Testa a estrutura do pacote"""
        logger.info("ğŸ“ Testando estrutura do pacote...")
        
        required_items = [
            "EnvironmentDevDeepEvaluation",  # DiretÃ³rio do executÃ¡vel
            "config",                        # ConfiguraÃ§Ãµes
            "docs",                         # DocumentaÃ§Ã£o
            "LEIA-ME.txt",                  # Arquivo de informaÃ§Ãµes
        ]
        
        optional_items = [
            "data",                         # Dados iniciais
            "Iniciar_Environment_Dev.bat", # Script Windows
            "iniciar_environment_dev.sh",  # Script Linux/Mac
        ]
        
        missing_required = []
        missing_optional = []
        
        for item in required_items:
            item_path = self.package_dir / item
            if not item_path.exists():
                missing_required.append(item)
            else:
                logger.info(f"  âœ… Encontrado: {item}")
        
        for item in optional_items:
            item_path = self.package_dir / item
            if not item_path.exists():
                missing_optional.append(item)
            else:
                logger.info(f"  âœ… Encontrado: {item}")
        
        test_passed = len(missing_required) == 0
        self.test_results["tests"]["package_structure"] = {
            "status": "pass" if test_passed else "fail",
            "missing_required": missing_required,
            "missing_optional": missing_optional
        }
        
        if test_passed:
            logger.info("  âœ… Estrutura do pacote OK")
        else:
            logger.error(f"  âŒ Itens obrigatÃ³rios ausentes: {missing_required}")
        
        return test_passed
    
    def test_executable_exists(self):
        """Testa se o executÃ¡vel existe"""
        logger.info("ğŸ”§ Testando existÃªncia do executÃ¡vel...")
        
        exe_dir = self.package_dir / "EnvironmentDevDeepEvaluation"
        exe_file = exe_dir / "EnvironmentDevDeepEvaluation.exe"
        exe_file_linux = exe_dir / "EnvironmentDevDeepEvaluation"
        
        exe_exists = exe_file.exists() or exe_file_linux.exists()
        
        if exe_exists:
            exe_path = exe_file if exe_file.exists() else exe_file_linux
            exe_size = exe_path.stat().st_size / (1024 * 1024)  # MB
            logger.info(f"  âœ… ExecutÃ¡vel encontrado: {exe_path.name}")
            logger.info(f"  ğŸ“Š Tamanho: {exe_size:.1f} MB")
        else:
            logger.error("  âŒ ExecutÃ¡vel nÃ£o encontrado")
        
        self.test_results["tests"]["executable_exists"] = {
            "status": "pass" if exe_exists else "fail",
            "executable_path": str(exe_path) if exe_exists else None,
            "size_mb": f"{exe_size:.1f}" if exe_exists else None
        }
        
        return exe_exists
    
    def test_configuration_files(self):
        """Testa se os arquivos de configuraÃ§Ã£o existem"""
        logger.info("âš™ï¸ Testando arquivos de configuraÃ§Ã£o...")
        
        config_dir = self.package_dir / "config"
        required_configs = [
            "components",  # DiretÃ³rio de componentes
        ]
        
        missing_configs = []
        for config in required_configs:
            config_path = config_dir / config
            if not config_path.exists():
                missing_configs.append(config)
            else:
                if config_path.is_dir():
                    file_count = len(list(config_path.glob("*.yaml")))
                    logger.info(f"  âœ… {config}: {file_count} arquivos YAML")
                else:
                    logger.info(f"  âœ… {config}: arquivo encontrado")
        
        test_passed = len(missing_configs) == 0
        self.test_results["tests"]["configuration_files"] = {
            "status": "pass" if test_passed else "fail",
            "missing_configs": missing_configs
        }
        
        if test_passed:
            logger.info("  âœ… Arquivos de configuraÃ§Ã£o OK")
        else:
            logger.error(f"  âŒ ConfiguraÃ§Ãµes ausentes: {missing_configs}")
        
        return test_passed
    
    def test_documentation(self):
        """Testa se a documentaÃ§Ã£o existe"""
        logger.info("ğŸ“š Testando documentaÃ§Ã£o...")
        
        docs_dir = self.package_dir / "docs"
        readme_file = self.package_dir / "LEIA-ME.txt"
        
        docs_exist = docs_dir.exists()
        readme_exists = readme_file.exists()
        
        if docs_exist:
            doc_count = len(list(docs_dir.glob("*.md")))
            logger.info(f"  âœ… DocumentaÃ§Ã£o: {doc_count} arquivos")
        else:
            logger.warning("  âš  DiretÃ³rio de documentaÃ§Ã£o nÃ£o encontrado")
        
        if readme_exists:
            readme_size = readme_file.stat().st_size
            logger.info(f"  âœ… LEIA-ME.txt: {readme_size} bytes")
        else:
            logger.warning("  âš  LEIA-ME.txt nÃ£o encontrado")
        
        test_passed = docs_exist or readme_exists
        self.test_results["tests"]["documentation"] = {
            "status": "pass" if test_passed else "fail",
            "docs_dir_exists": docs_exist,
            "readme_exists": readme_exists
        }
        
        return test_passed
    
    def test_startup_scripts(self):
        """Testa os scripts de inicializaÃ§Ã£o"""
        logger.info("ğŸš€ Testando scripts de inicializaÃ§Ã£o...")
        
        bat_script = self.package_dir / "Iniciar_Environment_Dev.bat"
        sh_script = self.package_dir / "iniciar_environment_dev.sh"
        
        bat_exists = bat_script.exists()
        sh_exists = sh_script.exists()
        
        if bat_exists:
            logger.info("  âœ… Script Windows (.bat) encontrado")
        else:
            logger.warning("  âš  Script Windows nÃ£o encontrado")
        
        if sh_exists:
            logger.info("  âœ… Script Linux/Mac (.sh) encontrado")
            # Verificar se Ã© executÃ¡vel
            try:
                is_executable = os.access(sh_script, os.X_OK)
                if is_executable:
                    logger.info("  âœ… Script Linux/Mac Ã© executÃ¡vel")
                else:
                    logger.warning("  âš  Script Linux/Mac nÃ£o Ã© executÃ¡vel")
            except:
                pass
        else:
            logger.warning("  âš  Script Linux/Mac nÃ£o encontrado")
        
        test_passed = bat_exists or sh_exists
        self.test_results["tests"]["startup_scripts"] = {
            "status": "pass" if test_passed else "fail",
            "bat_exists": bat_exists,
            "sh_exists": sh_exists
        }
        
        return test_passed
    
    def test_executable_dependencies(self):
        """Testa se o executÃ¡vel tem todas as dependÃªncias"""
        logger.info("ğŸ”— Testando dependÃªncias do executÃ¡vel...")
        
        exe_dir = self.package_dir / "EnvironmentDevDeepEvaluation"
        
        # Verificar se existem bibliotecas essenciais
        essential_libs = [
            "_internal",  # DiretÃ³rio interno do PyInstaller
        ]
        
        missing_libs = []
        for lib in essential_libs:
            lib_path = exe_dir / lib
            if not lib_path.exists():
                missing_libs.append(lib)
            else:
                if lib_path.is_dir():
                    file_count = len(list(lib_path.rglob("*")))
                    logger.info(f"  âœ… {lib}: {file_count} arquivos")
                else:
                    logger.info(f"  âœ… {lib}: encontrado")
        
        # Calcular tamanho total
        total_size = 0
        file_count = 0
        for file_path in exe_dir.rglob("*"):
            if file_path.is_file():
                total_size += file_path.stat().st_size
                file_count += 1
        
        size_mb = total_size / (1024 * 1024)
        logger.info(f"  ğŸ“Š Total: {file_count} arquivos, {size_mb:.1f} MB")
        
        test_passed = len(missing_libs) == 0
        self.test_results["tests"]["executable_dependencies"] = {
            "status": "pass" if test_passed else "fail",
            "missing_libs": missing_libs,
            "total_files": file_count,
            "total_size_mb": f"{size_mb:.1f}"
        }
        
        return test_passed
    
    def run_quick_executable_test(self):
        """Executa um teste rÃ¡pido do executÃ¡vel"""
        logger.info("âš¡ Executando teste rÃ¡pido do executÃ¡vel...")
        
        exe_dir = self.package_dir / "EnvironmentDevDeepEvaluation"
        exe_file = exe_dir / "EnvironmentDevDeepEvaluation.exe"
        exe_file_linux = exe_dir / "EnvironmentDevDeepEvaluation"
        
        exe_path = exe_file if exe_file.exists() else exe_file_linux
        
        if not exe_path.exists():
            logger.error("  âŒ ExecutÃ¡vel nÃ£o encontrado para teste")
            return False
        
        try:
            # Tentar executar com --help ou --version (se suportado)
            # Como nÃ£o sabemos se o executÃ¡vel suporta esses parÃ¢metros,
            # vamos apenas verificar se ele pode ser executado
            logger.info("  â„¹ï¸ Teste de execuÃ§Ã£o nÃ£o implementado (requer GUI)")
            logger.info("  âœ… ExecutÃ¡vel existe e parece vÃ¡lido")
            
            self.test_results["tests"]["executable_test"] = {
                "status": "skip",
                "reason": "GUI application - manual test required"
            }
            
            return True
            
        except Exception as e:
            logger.error(f"  âŒ Erro no teste do executÃ¡vel: {e}")
            self.test_results["tests"]["executable_test"] = {
                "status": "fail",
                "error": str(e)
            }
            return False
    
    def generate_test_report(self):
        """Gera relatÃ³rio dos testes"""
        logger.info("ğŸ“Š Gerando relatÃ³rio de testes...")
        
        # Calcular estatÃ­sticas
        total_tests = len(self.test_results["tests"])
        passed_tests = sum(1 for test in self.test_results["tests"].values() 
                          if test["status"] == "pass")
        failed_tests = sum(1 for test in self.test_results["tests"].values() 
                          if test["status"] == "fail")
        skipped_tests = sum(1 for test in self.test_results["tests"].values() 
                           if test["status"] == "skip")
        
        # Determinar status geral
        if failed_tests == 0:
            overall_status = "pass"
        elif passed_tests > failed_tests:
            overall_status = "partial"
        else:
            overall_status = "fail"
        
        self.test_results["overall_status"] = overall_status
        self.test_results["statistics"] = {
            "total": total_tests,
            "passed": passed_tests,
            "failed": failed_tests,
            "skipped": skipped_tests
        }
        
        # Salvar relatÃ³rio
        report_file = self.deployment_dir / f"test_report_{int(time.time())}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(self.test_results, f, indent=2, ensure_ascii=False)
        
        logger.info(f"  âœ… RelatÃ³rio salvo: {report_file}")
        
        # Mostrar resumo
        print(f"\nğŸ“Š RESUMO DOS TESTES")
        print("=" * 30)
        print(f"Total de testes: {total_tests}")
        print(f"âœ… Passou: {passed_tests}")
        print(f"âŒ Falhou: {failed_tests}")
        print(f"â­ï¸ Pulado: {skipped_tests}")
        print(f"Status geral: {overall_status.upper()}")
        
        return overall_status
    
    def run_all_tests(self):
        """Executa todos os testes"""
        logger.info("ğŸ§ª Iniciando testes do pacote de deployment...")
        
        if not self.find_package_directory():
            return False
        
        tests = [
            ("Estrutura do Pacote", self.test_package_structure),
            ("ExecutÃ¡vel", self.test_executable_exists),
            ("ConfiguraÃ§Ãµes", self.test_configuration_files),
            ("DocumentaÃ§Ã£o", self.test_documentation),
            ("Scripts de InicializaÃ§Ã£o", self.test_startup_scripts),
            ("DependÃªncias", self.test_executable_dependencies),
            ("Teste RÃ¡pido", self.run_quick_executable_test),
        ]
        
        for test_name, test_func in tests:
            logger.info(f"\n--- {test_name} ---")
            try:
                test_func()
            except Exception as e:
                logger.error(f"âŒ Erro no teste '{test_name}': {e}")
                self.test_results["tests"][test_name.lower().replace(" ", "_")] = {
                    "status": "error",
                    "error": str(e)
                }
        
        # Gerar relatÃ³rio final
        overall_status = self.generate_test_report()
        
        return overall_status in ["pass", "partial"]

def main():
    """FunÃ§Ã£o principal"""
    print("ğŸ§ª Environment Dev Deep Evaluation - Teste de Deployment")
    print("=" * 60)
    
    tester = DeploymentTester()
    success = tester.run_all_tests()
    
    if success:
        print("\nâœ… TESTES CONCLUÃDOS COM SUCESSO!")
        print("O pacote de deployment estÃ¡ pronto para distribuiÃ§Ã£o.")
    else:
        print("\nâŒ ALGUNS TESTES FALHARAM!")
        print("Verifique os erros acima antes de distribuir o pacote.")
    
    return success

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)